#!/bin/bash

set -e

echo "ðŸš€ Starting LM Studio compatible server..."

# Default model URL for automatic download (Google Gemma 3n-E4B-it)
DEFAULT_MODEL_URL="https://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_K_M.gguf"
DEFAULT_MODEL_NAME="gemma-3n-E4B-it-Q4_K_M.gguf"

# Check if model exists, if not download a default one
if [ ! -f "$MODEL_PATH" ]; then
    echo "ðŸ“¦ No model found at $MODEL_PATH"
    echo "ðŸ”„ Auto-downloading default model for plug-and-play setup..."
    echo "ðŸ“ Model: Google Gemma 3n-E4B IT (Instruction Tuned)"
    echo "ðŸ“ Size: ~6GB (Q4_K_M quantization - good balance of quality and speed)"
    
    # Create models directory
    mkdir -p /opt/models
    
    # Download with progress
    echo "â¬‡ï¸ Downloading model... This may take a few minutes depending on your connection."
    if wget --progress=bar:force:noscroll -O "$MODEL_PATH" "$DEFAULT_MODEL_URL"; then
        echo "âœ… Model downloaded successfully!"
        echo "ðŸ“ Saved to: $MODEL_PATH"
    else
        echo "âŒ Failed to download model. Please check your internet connection."
        echo "ðŸ’¡ You can manually download a GGUF model and place it at $MODEL_PATH"
        exit 1
    fi
else
    echo "âœ… Found existing model at $MODEL_PATH"
fi

echo "ðŸ“¦ Model: $MODEL_PATH"
echo "ðŸŒ Host: $HOST"
echo "ðŸ”Œ Port: $PORT"
echo "ðŸŽ® GPU Layers: $GPU_LAYERS"
echo "ðŸ“– Context Size: $CONTEXT_SIZE"

# Check GPU availability
if command -v nvidia-smi &> /dev/null; then
    echo "ðŸŽ® NVIDIA GPU detected:"
    nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader,nounits
    GPU_ARGS="--n-gpu-layers $GPU_LAYERS"
else
    echo "ðŸ’» No GPU detected, using CPU"
    GPU_ARGS=""
fi

# Start the llama.cpp server with OpenAI API compatibility
exec ./server \
    --model "$MODEL_PATH" \
    --host "$HOST" \
    --port "$PORT" \
    --ctx-size "$CONTEXT_SIZE" \
    --api-key "none" \
    --threads $(nproc) \
    $GPU_ARGS \
    --verbose
