#!/bin/bash

set -e

echo "ğŸ Starting MLX-optimized server for macOS Apple Silicon..."

# Default model for automatic download (MedraN-E4B-Uncensored MLX format)
DEFAULT_MODEL_HF="drwlf/MedraN-E4B-Uncensored-MLX"
MODEL_PATH="/opt/models/mlx-model"

# Check if model exists, if not download it
if [ ! -d "$MODEL_PATH" ]; then
    echo "ğŸ“¦ No MLX model found at $MODEL_PATH"
    echo "ğŸ”„ Auto-downloading optimized MLX model for Apple Silicon..."
    echo "ğŸ“ Model: MedraN-E4B-Uncensored (Medical AI optimized, MLX format)"
    echo "ğŸ“ Size: ~6GB (specialized for medical and healthcare tasks)"
    
    # Create models directory
    mkdir -p /opt/models
    
    echo "â¬‡ï¸ Downloading MLX model... This may take a few minutes."
    python -c "
from huggingface_hub import snapshot_download
import os
snapshot_download(
    repo_id='$DEFAULT_MODEL_HF',
    local_dir='$MODEL_PATH',
    local_dir_use_symlinks=False
)
print('âœ… MLX model downloaded successfully!')
"
else
    echo "âœ… Found existing MLX model at $MODEL_PATH"
fi

echo "ğŸ“¦ Model: $MODEL_PATH"
echo "ğŸŒ Host: $HOST"
echo "ğŸ”Œ Port: $PORT"
echo "ğŸ Platform: macOS with Apple Metal GPU acceleration"

# Start the MLX server
exec python /opt/mlx-server.py
