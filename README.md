# ğŸ¥ MedraN - Medical Assistant

A powerful, self-hosted medical AI assistant designed for healthcare professionals and medical students. Features advanced document processing with OCR preprocessing, RAG (Retrieval-Augmented Generation) for medical literature, and multi-modal search functionality tailored for medical documentation and research.

## âœ¨ Features

### ğŸ¥ Medical-Specific Capabilities
- **ğŸ“– Medical Literature Processing**: Optimized for medical journals, research papers, and clinical documentation
- **ğŸ”¬ Research Assistant**: Query complex medical texts and get contextual answers
- **ğŸ“‹ Clinical Documentation**: Process patient notes, medical reports, and diagnostic imaging reports
- **ğŸ“ Medical Education**: Perfect for medical students and residents studying from textbooks and case studies
- **ğŸ” Symptom & Treatment Lookup**: Fast retrieval from medical knowledge bases

### ğŸ¤– Core AI Features
- **ğŸ” Advanced OCR Processing**: Automatically enhances text extraction from medical documents, including scanned medical texts and complex layouts
- **ğŸ“š RAG (Retrieval-Augmented Generation)**: Upload medical documents and get contextual, citation-backed answers
- **ğŸ§  Conversation Memory**: Maintains context across medical consultations and study sessions
- **ğŸ™ï¸ Voice Transcription**: Speech-to-text for medical dictation and note-taking (supports WAV, MP3, M4A, FLAC, OGG)
- **ğŸ–¼ï¸ Multi-modal Search**: Text and image retrieval from medical documents and imaging reports
- **ğŸŒ Web Interface**: Clean, responsive UI designed for healthcare professionals
- **ğŸ³ Docker-First**: Fully containerized for easy deployment in medical environments
- **ğŸ—ï¸ Multi-Architecture**: Builds natively on both AMD64 and ARM64 systems
- **ğŸ”§ Self-Hosted**: Complete control over sensitive medical data - fully HIPAA-compliant when properly configured

## ğŸš€ Quick Start

### Prerequisites

- **Docker** and **Docker Compose** installed
- **4GB+ RAM** recommended for optimal performance
- **Local LLM Server** (LM Studio, Ollama, or similar) running on port 1234
  - **OR** use our **containerized LM Studio** (see Containerized Setup below)

### 1. Clone & Run

```bash
git clone https://github.com/dralexlup/MedraN-Medical-Assistant.git
cd MedraN-Medical-Assistant

# ğŸ† SUPER EASY: One-command setup (Recommended for beginners)
./quick-start.sh

# OR Manual setup options:
# Option A: Use external LLM server (LM Studio, Ollama, etc.)
docker compose up --build -d

# Option B: Use containerized LM Studio (self-contained)
./docker-start.sh -f docker-compose.lmstudio.yml up --build -d
```

### 2. Access the System

ğŸš€ **Single Entry Point**: Everything accessible via port 3000!

- **Web UI**: http://localhost:3000/
- **API**: http://localhost:3000/api/...
- **Health Check**: http://localhost:3000/health
- **MinIO Console**: http://localhost:3000/minio-console/ (if needed for storage management)
- **ChromaDB**: http://localhost:3000/chroma/ (optional)

> **Note**: If MinIO console doesn't work perfectly through the proxy, you can enable direct access by temporarily adding `ports: ["9001:9001"]` to the minio service in docker-compose.yml and accessing it at http://localhost:9001

### 3. Upload Documents

1. Open the web UI at http://localhost:3000
2. Click "Upload Document" 
3. Select a PDF file
4. Wait for processing (OCR enhancement will automatically improve text extraction)
5. Start chatting with your documents!

## ğŸ¥ Medical Use Cases

### For Healthcare Professionals
- **ğŸ“‹ Clinical Decision Support**: Upload medical guidelines and get evidence-based recommendations
- **ğŸ”¬ Research Literature Review**: Process multiple research papers and extract key findings
- **ğŸ“Š Case Study Analysis**: Upload patient cases and get diagnostic insights
- **ğŸ’Š Drug Information**: Query pharmaceutical references for dosing, interactions, and contraindications

### For Medical Students & Residents
- **ğŸ“š Textbook Study Assistant**: Upload medical textbooks and get interactive Q&A
- **ğŸ¯ Exam Preparation**: Practice with case-based questions from your study materials
- **ğŸ§  Concept Clarification**: Get detailed explanations of complex medical concepts
- **ğŸ“– Literature Review**: Quickly extract relevant information from research papers

### Example Medical Queries
```
"What are the contraindications for this medication?"
"Summarize the key findings from this clinical trial"
"What are the differential diagnoses for these symptoms?"
"Explain the pathophysiology of this condition"
"What are the current treatment guidelines for this disease?"
```

## ğŸ› ï¸ Configuration

### Environment Variables

The system uses these key environment variables (configurable in `docker-compose.yml`):

```yaml
# LLM Configuration
OPENAI_BASE_URL: "http://host.docker.internal:1234/v1"  # Your local LLM server
OPENAI_CHAT_MODEL: "your-model-name"                     # Model name from your LLM server

# Model Configuration  
EMBEDDING_MODEL: "bge-m3"                                # Text embedding model
IMAGE_EMBEDDING_MODEL: "clip-ViT-L-14"                  # Image embedding model
OCR_MODEL: "microsoft/trocr-base-printed"               # OCR model for text extraction

# Storage
MINIO_BUCKET: "medrandocs"                              # Document storage bucket
MAX_CONTEXT_CHARS: "120000"                            # Maximum context window
```

### Supported LLM Servers

- **LM Studio**: Default configuration (port 1234)
- **Containerized LM Studio**: Fully self-contained (see below)
- **Ollama**: Change `OPENAI_BASE_URL` to `http://host.docker.internal:11434/v1`
- **OpenAI Compatible APIs**: Any OpenAI-compatible endpoint

### ğŸ³ Containerized Setup (Self-Contained)

For a fully self-contained setup without external dependencies:

1. **Download a model** in GGUF format:
   ```bash
   # Example: Download a medical-focused model
   mkdir -p models
   cd models
   # Download your preferred GGUF model (e.g., from HuggingFace)
   wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf
   mv llama-2-7b-chat.Q4_K_M.gguf model.gguf
   cd ..
   ```

2. **Start with containerized LM Studio**:
   ```bash
   # Automatically detects GPU support and starts everything
   ./docker-start.sh -f docker-compose.lmstudio.yml up --build -d
   ```

3. **Access the system**: Same as regular setup at http://localhost:3000

**Benefits:**
- âœ… No external LLM server required
- âœ… Automatic GPU acceleration (CUDA on Linux, CPU-optimized elsewhere)
- âœ… Consistent performance across environments
- âœ… Easier deployment and scaling

## ğŸ“‹ API Endpoints

### Core Functionality

- `GET /api/healthz` - Health check
- `POST /api/ingest` - Upload and process documents (with OCR enhancement)
- `POST /api/chat` - Chat with your knowledge base
- `POST /api/transcribe` - Voice-to-text transcription
- `GET /health` - System health check (reverse proxy)

### Example Usage

```bash
# Health check
curl http://localhost:3000/health
curl http://localhost:3000/api/healthz

# Upload a document
curl -X POST "http://localhost:3000/api/ingest" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@document.pdf" \
  -F "title=My Document"

# Chat with the document
curl -X POST "http://localhost:3000/api/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What are the main topics covered?",
    "user_id": "user123",
    "k": 6,
    "return_images": true,
    "remember": true
  }'
```

## ğŸ—ï¸ Architecture

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Nginx Proxy     â”‚ â†â”€â”€ Single Entry Point
                    â”‚ (Port 3000)     â”‚     http://localhost:3000
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼               â–¼               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Web UI        â”‚ â”‚   FastAPI    â”‚ â”‚   LLM Server    â”‚
    â”‚   (Internal)    â”‚ â”‚ (Internal)   â”‚ â”‚   (Port 1234)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚           â”‚           â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
           â”‚  ChromaDB  â”‚ â”‚  Redis  â”‚ â”‚  MinIO   â”‚
           â”‚ (Internal) â”‚ â”‚(Internal)â”‚ â”‚(Internal)â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Advanced Configuration

### Custom Models

You can customize the models by editing `docker-compose.yml`:

```yaml
environment:
  EMBEDDING_MODEL: "your-preferred-embedding-model"
  OCR_MODEL: "your-preferred-ocr-model"
  IMAGE_EMBEDDING_MODEL: "your-preferred-image-model"
```

### Resource Requirements

- **Minimal**: 2GB RAM, 2 CPU cores
- **Recommended**: 4GB RAM, 4 CPU cores
- **Production**: 8GB+ RAM, 8+ CPU cores

### GPU Support

The system **automatically detects and uses GPU acceleration** when available:
- **CUDA GPUs**: Automatically detected for NVIDIA cards
- **Apple Silicon (MPS)**: Automatically detected for M1/M2/M3 Macs
- **CPU Fallback**: Uses CPU when no GPU is available

For Docker GPU access (NVIDIA only), add to your `docker-compose.yml`:

```yaml
api:
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
```

**Note**: Apple Silicon GPU support works automatically without Docker configuration.

## ğŸ› Troubleshooting

### Common Issues

1. **"Cannot connect to LLM server" (httpx.ConnectError)**
   - Ensure LM Studio is running and **"Allow network access"** is enabled
   - Verify the model is loaded in LM Studio
   - Check that LM Studio is running on port 1234
   - On Linux: Use your host IP instead of `host.docker.internal`

2. **"CLIPConfig object has no attribute 'hidden_size'"**
   - This is fixed in the latest version with updated CLIP model
   - If still occurring, try: `docker compose down && docker compose up --build`

3. **Large file upload failures**
   - The system now supports uploads up to 500MB
   - Files that timeout during upload may need LM Studio to be started first

4. **Out of memory during model loading**
   - Increase Docker memory limits to 4GB+
   - GPU acceleration reduces memory usage significantly

5. **Slow document processing**
   - GPU acceleration now automatic when available
   - OCR processing is CPU-intensive on CPU-only systems

### Logs

```bash
# View all service logs
docker compose logs -f

# View specific service logs  
docker compose logs -f api
docker compose logs -f chroma
```

## ğŸ“Š Performance

### OCR Enhancement

The system automatically uses OCR when it detects that it can significantly improve text extraction:

- **Smart Enhancement**: Only uses OCR when beneficial (20%+ text improvement)
- **Fallback**: Works without OCR if models fail to load
- **Logging**: Monitor OCR usage via API logs

### Benchmarks

On a modern system (M1 MacBook Pro):
- **Document Ingestion**: ~2-3 pages/second with OCR
- **Query Response**: ~500ms average
- **Memory Usage**: ~1.5GB per service

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test with `docker compose up --build`
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™‹â€â™‚ï¸ Support

- **Issues**: Create an issue on GitHub
- **Documentation**: Check `DEPLOYMENT.md` for advanced deployment options
- **Community**: Join discussions in GitHub Discussions

## ğŸ¯ Roadmap

- [ ] Web UI improvements and dark mode
- [ ] Support for more document formats (DOCX, TXT, etc.)
- [ ] Advanced OCR models (table extraction, formula recognition)
- [ ] Multi-language support
- [ ] Plugin system for custom tools
- [ ] Cloud deployment templates (AWS, GCP, Azure)

---

**Made with â¤ï¸ for the open-source community**

*Self-hosted AI that respects your privacy and gives you full control over your data.*
