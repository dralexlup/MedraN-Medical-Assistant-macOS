# ğŸ MedraN Medical AI Assistant - macOS Apple Silicon

**Ultra-fast medical AI assistant optimized for Apple Silicon (M1/M2/M3) using LM Studio**

Experience **blazing-fast** medical AI responses on your Apple Silicon Mac! This setup leverages LM Studio's native Metal GPU acceleration for optimal performance.

## ğŸš€ **Why LM Studio for macOS?**

| Feature | LM Studio (This Setup) | Docker Containers |
|---------|------------------------|-------------------|
| **Performance** | âš¡ Native Metal GPU | âŒ Limited CPU |
| **Memory Usage** | ğŸ§  6-8GB efficient | ğŸ’¾ 12-16GB overhead |
| **Setup Time** | â±ï¸ 5 minutes | â±ï¸ 20+ minutes |
| **Reliability** | âœ… Native stability | âš ï¸ Container complexity |
| **Model Control** | ğŸ¯ Direct model management | âŒ Limited options |

## âœ¨ **Key Features**

### ğŸ **Apple Silicon Optimized**
- **LM Studio Native**: Direct Metal GPU acceleration
- **Zero Docker Overhead**: No containerization performance loss  
- **Model Flexibility**: Choose any GGUF model you prefer
- **Instant Responses**: True real-time medical AI conversations

### ğŸ¥ **Medical AI Capabilities**  
- **ğŸ“– Medical Literature**: Process journals, papers, clinical docs
- **ğŸ”¬ Research Assistant**: Query medical texts with context
- **ğŸ’Š Drug Information**: Medication lookups and interactions
- **ğŸ“ Medical Education**: Interactive learning for students/residents
- **ğŸ§  Conversation Memory**: Maintains context across sessions

## ğŸ **Quick Start (5 minutes)**

### Prerequisites
- **macOS** with Apple Silicon (M1/M2/M3)
- **16GB+ RAM** recommended
- **LM Studio** (we'll install this)

### Step 1: Install LM Studio
```bash
# Download and install LM Studio
open https://lmstudio.ai/

# Or install via Homebrew
brew install --cask lm-studio
```

### Step 2: Download Medical Model
1. **Open LM Studio**
2. **Go to "Discover" tab**
3. **Search for**: `MedraN-E4B-Uncensored` or `Llama-3.1-8B-Instruct`
4. **Download** your preferred medical model (GGUF format)
5. **Recommended**: 
   - `drwlf/MedraN-E4B-Uncensored-GGUF` (6GB)
   - `microsoft/Phi-3-mini-128k-instruct-gguf` (2.3GB - lighter option)

### Step 3: Start LM Studio Server
1. **Go to "Local Server" tab** in LM Studio
2. **Load your downloaded model**
3. **Click "Start Server"**
4. **Note the server URL**: Usually `http://localhost:1234`

### Step 4: Start MedraN Assistant
```bash
# Clone this repository
git clone https://github.com/dralexlup/MedraN-Medical-Assistant-macOS.git
cd MedraN-Medical-Assistant-macOS

# Start the medical assistant (connects to LM Studio automatically)
docker-compose up -d

# Access your medical AI at http://localhost:3000 ğŸ‰
```

## ğŸ› ï¸ **Configuration**

### LM Studio Settings
**Recommended settings for medical use:**
- **Temperature**: 0.3 (more focused responses)
- **Max Tokens**: 2048
- **Context Length**: 32K+ (for long medical documents)
- **GPU Acceleration**: âœ… Enabled (Metal)

### Model Recommendations

#### ğŸ¥ **Medical-Specialized Models**
1. **MedraN-E4B-Uncensored** (6GB) - Best for medical accuracy
2. **OpenBioLLM-8B** (4.6GB) - Biomedical research focused
3. **MedAlpaca-13B** (7.3GB) - Clinical decision support

#### ğŸ’¡ **General Models (Medical-Capable)**
1. **Llama-3.1-8B-Instruct** (4.6GB) - Excellent general medical knowledge
2. **Phi-3-mini-128k** (2.3GB) - Fast, efficient, good medical reasoning
3. **Qwen2-7B-Instruct** (4.1GB) - Strong multilingual medical support

## ğŸ“Š **Performance Expectations**

### Apple Silicon Performance
**M1/M2/M3 MacBook (16GB RAM):**
- **Inference Speed**: 50-150 tokens/second
- **Response Time**: 1-3 seconds for medical queries
- **Memory Usage**: 6-10GB total system usage
- **GPU Utilization**: 70-90% Metal GPU

**Mac Studio/Mac Pro:**
- **Inference Speed**: 100-200+ tokens/second  
- **Response Time**: 0.5-2 seconds
- **Concurrent Users**: 2-5 simultaneous conversations

## ğŸ¥ **Medical Use Cases**

### Example Performance
**Query**: *"What are the contraindications for ACE inhibitors in diabetic patients with chronic kidney disease?"*

**LM Studio Response**: ~2-3 seconds with detailed, accurate medical information

### Perfect For:
- **ğŸ‘¨â€âš•ï¸ Healthcare Professionals**: Quick clinical decision support
- **ğŸ“ Medical Students**: Interactive learning and case studies  
- **ğŸ”¬ Medical Researchers**: Literature analysis and hypothesis generation
- **ğŸ’Š Pharmacists**: Drug interaction and dosing guidance

## ğŸ”§ **Troubleshooting**

### LM Studio Issues
```bash
# Check if LM Studio server is running
curl http://localhost:1234/v1/models

# If server not responding:
# 1. Restart LM Studio
# 2. Reload your model
# 3. Check server port (should be 1234)
```

### Performance Optimization
- **Close unnecessary apps** to free up RAM
- **Use smaller models** if memory is limited (Phi-3-mini is excellent)
- **Increase context length** for processing long medical documents
- **Enable GPU acceleration** in LM Studio settings

### Memory Management
- **Monitor Activity Monitor** â†’ Memory tab
- **Quit Chrome/Safari tabs** to free RAM for the model
- **Use "Memory Pressure" indicator** to optimize performance

## ğŸ†š **LM Studio vs Docker Comparison**

### âœ… **LM Studio Advantages**
- **ğŸš€ 5-10x Faster**: Native Metal GPU performance
- **ğŸ§  50% Less Memory**: No container overhead
- **ğŸ¯ Model Control**: Easy model switching and management
- **ğŸ”§ Simple Setup**: No complex Docker configurations
- **ğŸ Apple Optimized**: Built specifically for macOS

### âŒ **Docker Limitations on macOS**
- **ğŸŒ Slower Performance**: CPU-only inference in containers
- **ğŸ’¾ Memory Hungry**: 12-16GB total usage vs 6-8GB with LM Studio
- **âš™ï¸ Complex Setup**: Multiple containers and configurations
- **ğŸ”’ Limited GPU**: No proper Metal acceleration in containers

## ğŸ›‘ **Stopping the System**

```bash
# Stop MedraN services
docker-compose down

# Stop LM Studio server (in LM Studio app)
# Go to "Local Server" tab â†’ Click "Stop Server"
```

## ğŸ”„ **Model Updates**

To try different models:
1. **Download new model** in LM Studio
2. **Stop current server**
3. **Load new model**
4. **Start server again**
5. **MedraN will automatically use the new model!**

## ğŸ“± **System Requirements**

### Minimum Requirements
- **Apple Silicon Mac** (M1/M2/M3)
- **8GB RAM** (will work but slower)
- **20GB free storage** (for models)

### Recommended Setup
- **16GB+ RAM** for best performance
- **32GB RAM** for multiple large models
- **50GB+ storage** for model collection

## ğŸ¤ **Contributing**

1. Fork this repository
2. Test changes with your LM Studio setup
3. Include performance benchmarks
4. Share model recommendations

## ğŸ“„ **License**

MIT License

## ğŸ™‹â€â™‚ï¸ **Support**

- **Issues**: GitHub issues for setup problems
- **Models**: Share your favorite medical models
- **Performance**: Include your Mac specs and model used
- **LM Studio**: Check [LM Studio docs](https://lmstudio.ai/docs) for app-specific issues

---

**Made with â¤ï¸ for Apple Silicon medical professionals**

*Experience medical AI at native Apple Silicon speeds!*
